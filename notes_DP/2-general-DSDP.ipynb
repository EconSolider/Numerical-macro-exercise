{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [Introduction](#introduction)\n",
    "    - [DSDP problem](#dsdp-problem)\n",
    "    - [Policies](#policies)\n",
    "- [Formal Definition](#formal-definition)\n",
    "    - [Structural of DSDP](#structure-of-dsdp-input)\n",
    "    - [Solution of DSDP](#solution-of-dsdp-output)\n",
    "- [Value and Optimality (Property of DSDP)](#value-and-optimality)\n",
    "    - [Two Operators](#two-operators)\n",
    "    - [Principle of Optimality](#the-bellman-equation-and-the-principle-of-optimality)\n",
    "- [Solving DSDP](#solving-dsdps)\n",
    "    - [Value Funciton Iteration](#value-function-iteration)\n",
    "    - [Policy iteration](#policy-iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous lecture, we had a brief introduction to the interesting aspects of DSDP through the McCall search model.\n",
    "\n",
    "In this lecture, we will introduce the general structure of DSDP and discuss two methods for solving DSDP:\n",
    "- Value Function Iteration\n",
    "- Policy Function Iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSDP Problem\n",
    "\n",
    "A discrete DP is a maximization problem with an objective function of the form\n",
    "\n",
    "<a id='equation-dp-objective'></a>\n",
    "$$\n",
    "\\sum_{t = 0}^{\\infty} \\beta^t \\mathbb{E} r(s_t, a_t) \\tag{1} \\\\\n",
    "\\text{s.t.} \\quad a_t \\in A(s_t)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ s_t $ is the state variable  \n",
    "- $ a_t $ is the action  \n",
    "- $ A(s_t) $ is the set of feasible actions\n",
    "- $ \\beta $ is a discount factor  \n",
    "- $ r(s_t, a_t) $ is interpreted as a current reward when the state is $ s_t $ and the action chosen is $ a_t $  \n",
    "\n",
    "In the optimization problem, the expectation $\\mathbb{E}$ is built on transition probabilities $ Q(s_t, a_t, s_{t+1}) $.\n",
    "\n",
    "Each pair $ (s_t, a_t) $ pins down transition probabilities $ Q(s_t, a_t, s_{t+1}) $ for the next period state $ s_{t+1} $. **Thus, actions influence not only current rewards but also the future time path of the state.**\n",
    "\n",
    "Examples:\n",
    "\n",
    "- consuming today vs saving and accumulating assets  \n",
    "- accepting a job offer today vs seeking a better one in the future  \n",
    "- exercising an option now vs waiting  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies\n",
    "\n",
    "**The most fruitful way to think about solutions to discrete DP problems is to compare *policies*.**\n",
    "\n",
    "In general, **a policy is a randomized map from past actions and states to current action.**\n",
    "\n",
    "In the setting formalized below, it suffices to consider so-called **stationary Markov policies**, which consider only the current state.\n",
    "\n",
    "In particular, a **stationary Markov policy is a time-invariant map $ \\sigma $ from states to actions**\n",
    "\n",
    "- $ a_t = \\sigma(s_t) $ indicates that $ a_t $ is the action to be taken in state $ s_t $  \n",
    "\n",
    "It is known that, for any arbitrary policy, there exists a stationary Markov policy that dominates it at least weakly.\n",
    "\n",
    "Hereafter, stationary Markov policies are referred to simply as policies.\n",
    "\n",
    "**The aim is to find an optimal policy, in the sense of one that maximizes [(1)](#equation-dp-objective).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal Definition\n",
    "\n",
    "### Structure of DSDP (Input):\n",
    "\n",
    "- A finite set of **states**: \n",
    "  $$ S = \\{0, \\ldots, n-1\\} $$\n",
    "\n",
    "- A finite set of **feasible actions**:\n",
    "  $$ A(s), s \\in S$$\n",
    "\n",
    "- A **reward function** \n",
    "  $$ \n",
    "  r\\colon \\mathit{SA} \\to \\mathbb{R} \\\\\n",
    "  \\text{where $SA$ is set of feasible state-action pairs:} \\\\\n",
    "  \\mathit{SA} := \\{(s, a) \\mid s \\in S, \\; a \\in A(s)\\}\n",
    "  $$  \n",
    "\n",
    "- A **transition probability function** \n",
    "  $$ Q\\colon \\mathit{SA} \\to \\Delta(S) \\\\\n",
    "  \\text{where $ \\Delta(S) $ is the set of probability distributions over $ S $.} $$  \n",
    "\n",
    "- A **discount factor** \n",
    "  $$ \\beta \\in [0, 1) $$ \n",
    "\n",
    "\n",
    "### Solution of DSDP (Output):\n",
    "\n",
    "- **Policy function**:  \n",
    "  $$ \\sigma\\colon S \\to A \\\\\n",
    "  \\text{where $A$ is the union of all feasible action sets:} \\\\\n",
    "  A := \\bigcup_{s \\in S} A(s) = \\{0, \\ldots, m-1\\}\n",
    "  $$\n",
    "\n",
    "- Policy $\\sigma$ is called **feasible policy**  \n",
    "  - If it maps states into feasible action set:\n",
    "   $ \\sigma(s) \\in A(s), \\forall s \\in S $  \n",
    "  - Define **set of all feasible polices** as $\\Sigma$.\n",
    "\n",
    "- If a decision-maker uses  a policy $ \\sigma \\in \\Sigma $, then\n",
    "  - the current reward at time $ t $ is $ r(s_t, \\sigma(s_t)) $  \n",
    "  - the probability that $ s_{t+1} = s' $ is $ Q(s_t, \\sigma(s_t), s') $  \n",
    "\n",
    "- For each $ \\sigma \\in \\Sigma $, define\n",
    "  - $ r_{\\sigma} $ by $ r_{\\sigma}(s) := r(s, \\sigma(s)) $ \n",
    "  - $ Q_{\\sigma} $ by $ Q_{\\sigma}(s, s') := Q(s, \\sigma(s), s') $  \n",
    "    where $Q_{\\sigma}$ is the probability matrix that satisfies:\n",
    "    - $\\sum_{s' \\in S} Q_{\\sigma}(s,s') = 1 $\n",
    "    - $Q_{\\sigma}(s,s')>0, \\forall s,s' \\in S$\n",
    "\n",
    "- If we think of $ r_\\sigma $ as a column vector, then so is $ Q_\\sigma^t r_\\sigma $, and the $ s $-th row of it can be expressed as\n",
    "\n",
    "  <a id='equation-ddp-expec'></a>\n",
    "  $$\n",
    "  (Q_\\sigma^t r_\\sigma)(s) = \\mathbb E [ r(s_t, \\sigma(s_t)) \\mid s_0 = s ]\n",
    "  \\quad \\text{when } \\{s_t\\} \\sim Q_\\sigma \\tag{2}\n",
    "  $$\n",
    "  \n",
    "  - $\\{s_t\\} \\sim Q_\\sigma $ means that the state is generated by stochastic matrix $ Q_\\sigma $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value and Optimality\n",
    "\n",
    "- Let $ v_{\\sigma}(s) $ denote the discounted sum of expected rewards from policy $ \\sigma $ when the initial state is $s$.\n",
    "    - To calculate this quantity we use [(1)](#equation-dp-objective) and [(2)](#equation-ddp-expec) to get\n",
    "    $$\n",
    "    v_{\\sigma}(s) = \\sum_{t=0}^{\\infty} \\beta^t (Q_{\\sigma}^t r_{\\sigma})(s)\n",
    "    \\qquad (s \\in S)\n",
    "    $$\n",
    "\n",
    "    - $v_{\\sigma}(s)$ is called the **policy value function for the policy $ \\sigma $**.\n",
    "\n",
    "- The **optimal value function (value function)** $ v^*\\colon S \\to \\mathbb{R} $ is defined by\n",
    "\n",
    "    $$\n",
    "    v^*(s) = \\max_{\\sigma \\in \\Sigma} v_{\\sigma}(s) \\qquad (s \\in S)\n",
    "    $$\n",
    "\n",
    "- A policy $ \\sigma \\in \\Sigma $ is called **optimal** if  \n",
    "    $$ v_{\\sigma}(s) = v^*(s), \\forall s \\in S $$\n",
    "\n",
    "- Given any $ w \\colon S \\to \\mathbb R $, a policy $ \\sigma \\in \\Sigma $ is called **$ w $-greedy** if\n",
    "    $$\n",
    "    \\sigma(s) \\in \\operatorname*{arg\\,max}_{a \\in A(s)}\n",
    "    \\left\\{\n",
    "        r(s, a) +\n",
    "        \\beta \\sum_{s' \\in S} w(s') Q(s, a, s')\n",
    "    \\right\\}\n",
    "    \\qquad (s \\in S)\n",
    "    $$\n",
    "    - As discussed in detail below, optimal policies are precisely those that are $ v^* $-greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of McCall Search Model\n",
    "\n",
    "To deepen our understanding, we use the McCall model from the previous lecture as an example to observe how its elements correspond to the standard structure of DSDP.\n",
    "\n",
    "#### Inputs\n",
    "- **Set of states $ S $:**  \n",
    "    - Possible values of wage offers: $\\mathbb{W}$\n",
    "\n",
    "- **Set of feasible actions $ A(s), s \\in S $:**\n",
    "    - For all $w \\in \\mathbb{W}$, $A(w) = \\{ \\text{Accept}, \\text{Refuse} \\}$\n",
    "\n",
    "- **Reward function $r: SA \\to \\mathbb{R}$:**\n",
    "    - $r(w, \\text{Accept}) = w$\n",
    "    - $r(w, \\text{Refuse}) = c$\n",
    "\n",
    "- **Transition probability function $Q: SA \\to \\Delta(S)$:**\n",
    "    - $Q(w, \\text{Accept}, w') = Q(w, \\text{Refuse}, w') = q(w')$\n",
    "    - Note that the distribution of wages is purely random and not affected by states and actions.\n",
    "\n",
    "#### Outputs\n",
    "- **Policy function $\\sigma: S \\to A$:**\n",
    "    - $\\sigma(w) = \\text{Accept}$ if $w > \\bar{w}$\n",
    "    - $\\sigma(w) = \\text{Refuse}$ if $w < \\bar{w}$\n",
    "    - where $\\bar{w}$ is the reservation wage\n",
    "\n",
    "- **Value function for policy $v_{\\sigma}: S \\to \\mathbb{R}$:**\n",
    "    - $v_{\\text{Accept}} = \\frac{w}{1 - \\beta}$\n",
    "    - $v_{\\text{Refuse}} = c + \\beta \\sum_{t=1}^{\\infty} q(w_t) r(w_t,\\sigma(w_t))$\n",
    "\n",
    "- **Optimal value function $v^*: S \\to \\mathbb{R}$:**\n",
    "    - $v^* = \\max \\{ v_{\\text{Accept}}, v_{\\text{Refuse}} \\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Operators\n",
    "\n",
    "- The **Bellman operator** $ T\\colon \\mathbb{R}^S \\to \\mathbb{R}^S $\n",
    "  is defined by\n",
    "  $$\n",
    "  (T v)(s) = \\max_{a \\in A(s)}\n",
    "  \\left\\{\n",
    "      r(s, a) + \\beta \\sum_{s' \\in S} v(s') Q(s, a, s')\n",
    "  \\right\\}\n",
    "  \\qquad (s \\in S)\n",
    "  $$\n",
    "\n",
    "- For any policy function $ \\sigma \\in \\Sigma $, the **policy operator** $ T_{\\sigma}\\colon \\mathbb{R}^S \\to \\mathbb{R}^S $ is defined by  \n",
    "  $$\n",
    "  (T_{\\sigma} v)(s) = r(s, \\sigma(s)) +\n",
    "      \\beta \\sum_{s' \\in S} v(s') Q(s, \\sigma(s), s')\n",
    "  \\qquad (s \\in S)\n",
    "  $$\n",
    "\n",
    "  - This can be written more succinctly in operator notation as\n",
    "    $$\n",
    "    T_{\\sigma} v = r_{\\sigma} + \\beta Q_{\\sigma} v\n",
    "    $$\n",
    "\n",
    "- **Properties** (we skip trivial proofs)\n",
    "  - **monotone**  \n",
    "    $ v \\leq w $  implies $ Tv \\leq Tw $ pointwise on $ S $, and\n",
    "    similarly for $ T_\\sigma $  \n",
    "\n",
    "  - **contraction mapping**  \n",
    "    $ \\lVert Tv - Tw \\rVert \\leq \\beta \\lVert v - w \\rVert $ and similarly for $ T_\\sigma $, where $ \\lVert \\cdot\\rVert $ is the max norm  \n",
    "\n",
    "  - For any policy $ \\sigma $, its value $ v_{\\sigma} $ is the **unique fixed point** of $ T_{\\sigma} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation and the Principle of Optimality\n",
    "\n",
    "The main principle of the theory of dynamic programming is that\n",
    "\n",
    "- **the optimal value function $ v^* $ is a unique solution to the Bellman equation**  \n",
    "    $$\n",
    "    v(s) = \\max_{a \\in A(s)}\n",
    "        \\left\\{\n",
    "            r(s, a) + \\beta \\sum_{s' \\in S} v(s') Q(s, a, s')\n",
    "        \\right\\}\n",
    "    \\qquad (s \\in S)\n",
    "    $$\n",
    "    or in other words, $ v^* $ is the unique fixed point of $ T $.\n",
    "\n",
    "- **$ \\sigma^* $ is an optimal policy function if and only if it is $ v^* $-greedy**  \n",
    "    By the definition of greedy policies given above, this means that\n",
    "\n",
    "    $$\n",
    "    \\sigma^*(s) \\in \\operatorname*{arg\\,max}_{a \\in A(s)}\n",
    "        \\left\\{\n",
    "        r(s, a) + \\beta \\sum_{s' \\in S} v^*(s') Q(s, a, s')\n",
    "        \\right\\}\n",
    "    \\qquad (s \\in S)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving DSDPs\n",
    "\n",
    "Now that the theory has been set out, let’s turn to solution methods.\n",
    "\n",
    "The code for solving discrete DPs is available in [ddp.py](https://quanteconpy.readthedocs.io/en/latest/markov/ddp.html) from the [QuantEcon.py](http://quantecon.org/quantecon-py) code library. `DiscreteDP`\n",
    "\n",
    "It implements the three most important solution methods for discrete dynamic programs, namely\n",
    "\n",
    "- value function iteration  \n",
    "- policy function iteration  \n",
    "- modified policy function iteration (skip for now)  \n",
    "\n",
    "\n",
    "Let’s briefly review these algorithms and their implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function Iteration\n",
    "\n",
    "This algorithm uses the fact that the Bellman operator $ T $ is a contraction mapping with fixed point $ v^* $.\n",
    "\n",
    "Hence, iterative application of $ T $ to any initial function $ v^0 \\colon S \\to \\mathbb R $ converges to $ v^* $.\n",
    "\n",
    "The `DiscreteDP` value iteration method implements value function iteration as\n",
    "follows\n",
    "\n",
    "1. Choose any $ v^0 \\in \\mathbb{R}^n $, and specify $ \\varepsilon > 0 $; set $ i = 0 $.  \n",
    "1. Compute $ v^{i+1} = T v^i $.  \n",
    "1. If $ \\lVert v^{i+1} - v^i\\rVert <  [(1 - \\beta) / (2\\beta)] \\varepsilon $,\n",
    "  then go to step 4; otherwise, set $ i = i + 1 $ and go to step 2.  \n",
    "1. Compute a $ v^{i+1} $-greedy policy $ \\sigma $, and return $ v^{i+1} $ and $ \\sigma $.  \n",
    "\n",
    "\n",
    "Given $ \\varepsilon > 0 $, the value iteration algorithm\n",
    "\n",
    "- terminates in a finite number of iterations  \n",
    "- returns an $ \\varepsilon/2 $-approximation of the optimal value function and an $ \\varepsilon $-optimal policy function (unless `iter_max` is reached)  \n",
    "\n",
    "\n",
    "(While not explicit, in the actual implementation each algorithm is\n",
    "terminated if the number of iterations reaches `iter_max`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "This routine, also known as **Howard’s policy improvement algorithm**, exploits more closely the particular structure of a discrete DP problem.\n",
    "\n",
    "Each iteration consists of\n",
    "1. A policy evaluation step that computes the value $ v_{\\sigma} $ of a policy $ \\sigma $ by solving the linear equation $ v = T_{\\sigma} v $.  \n",
    "1. A policy improvement step that computes a $ v_{\\sigma} $-greedy policy.  \n",
    "\n",
    "The `DiscreteDP` policy iteration method runs as follows\n",
    "\n",
    "1. Choose any $ v^0 \\in \\mathbb{R}^n $ and compute a $ v^0 $-greedy policy $ \\sigma^0 $; set $ i = 0 $.  \n",
    "1. Compute the value $ v_{\\sigma^i} $ by solving\n",
    "  the equation $ v = T_{\\sigma^i} v $.  \n",
    "1. Compute a $ v_{\\sigma^i} $-greedy policy\n",
    "  $ \\sigma^{i+1} $; let $ \\sigma^{i+1} = \\sigma^i $ if\n",
    "  possible.  \n",
    "1. If $ \\sigma^{i+1} = \\sigma^i $, then return $ v_{\\sigma^i} $\n",
    "  and $ \\sigma^{i+1} $; otherwise, set $ i = i + 1 $ and go to\n",
    "  step 2.  \n",
    "\n",
    "\n",
    "The policy iteration algorithm terminates in a finite number of\n",
    "iterations.\n",
    "\n",
    "It returns an optimal value function and an optimal policy function (unless `iter_max` is reached)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
